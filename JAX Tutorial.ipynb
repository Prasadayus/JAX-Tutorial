{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4d011a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#J-JIT Complilation-Just in Time\n",
    "#A-Automatic Differentiation\n",
    "#X-XLA(Acclelerated Linear Algebra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68324d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 7 9]\n",
      "[1.        1.4142135 1.7320508]\n",
      "[[1]\n",
      " [2]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "#JAX as NumPy\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "a=jnp.array([1,2,3])\n",
    "b=jnp.array([4,5,6])\n",
    "\n",
    "print(a+b)\n",
    "print(jnp.sqrt(a))\n",
    "print(a.reshape(-1,1))\n",
    "#but in jax array are immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f903b535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunction(x):\n",
    "    return jnp.where(x%2==0,x/2,3*x+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3857ebbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000547399977222085\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "arr=jnp.arange(10)\n",
    "start=time.perf_counter()\n",
    "myfunction(arr)\n",
    "end=time.perf_counter()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ce9a658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00025809998624026775\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "arr=jnp.arange(10)\n",
    "start=time.perf_counter()\n",
    "myfunction(arr).block_until_ready()\n",
    "end=time.perf_counter()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f966de7",
   "metadata": {},
   "source": [
    "The difference between these two code snippets lies in JAX's lazy evaluation model. JAX uses Just-In-Time (JIT) compilation and asynchronous execution on GPUs/TPUs, meaning computations might not execute immediately when called.\n",
    "\n",
    "### First Code:\n",
    "\n",
    "- myfunction(arr) does not force immediate execution.\n",
    "- JAX schedules the computation asynchronously (especially on GPUs/TPUs).\n",
    "- time.perf_counter() captures the time before execution is actually complete.\n",
    "- The reported time might be smaller than the actual execution time.\n",
    "üîπ Issue: The function execution may still be running when the timer stops!\n",
    "\n",
    "### Second Code:\n",
    "\n",
    "- myfunction(arr) starts execution.\n",
    "- .block_until_ready() forces JAX to complete the computation before proceeding.\n",
    "- The timer only stops after all computations are actually finished.\n",
    "- This gives the true execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4798d917",
   "metadata": {},
   "source": [
    "## Just-in-time compilation(JIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e54afc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without jit: [ 0.  4.  1. 10.  2. 16.  3. 22.  4. 28.]\n",
      "Execution Time: 0.03771480009891093\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "\n",
    "c\n",
    "\n",
    "arr = jnp.arange(10)\n",
    "\n",
    "# Measure execution time\n",
    "start = time.perf_counter()\n",
    "result = myfunction(arr).block_until_ready()   # Normal execution\n",
    "end = time.perf_counter()\n",
    "\n",
    "print(\"Without jit:\", result)\n",
    "print(\"Execution Time:\", end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "647c9a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With jit: [ 0.  4.  1. 10.  2. 16.  3. 22.  4. 28.]\n",
      "Execution Time: 0.00038650003261864185\n"
     ]
    }
   ],
   "source": [
    "myfunction_jit = jax.jit(myfunction)\n",
    "\n",
    "start = time.perf_counter()\n",
    "result_jit = myfunction_jit(arr).block_until_ready()  # JIT-compiled execution\n",
    "end = time.perf_counter()\n",
    "\n",
    "print(\"With jit:\", result_jit)\n",
    "print(\"Execution Time:\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8456d6c9",
   "metadata": {},
   "source": [
    "JAX provides the jax.jit (Just-In-Time compilation) decorator to speed up functions by compiling them with XLA (Accelerated Linear Algebra). This improves performance by optimizing computation and reducing Python overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5eb223",
   "metadata": {},
   "source": [
    "### Key Observations:\n",
    "- First Call Overhead: The first time you call a JIT-compiled function, it takes longer because JAX compiles it.\n",
    "- Subsequent Calls are Faster: After compilation, execution runs much faster since the function is now optimized.\n",
    "- Automatic Vectorization: jax.jit enables parallel execution and fuses operations efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21a0341",
   "metadata": {},
   "source": [
    "**Avoid jax.jit when:**\n",
    "\n",
    "Your function involves Python loops, print statements, or control flow that changes shape dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f6cd2d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:i32[10]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
      "    \u001b[39m\u001b[22m\u001b[22mb\u001b[35m:f32[10]\u001b[39m = pjit[\n",
      "      name=myfunction\n",
      "      jaxpr={ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; c\u001b[35m:i32[10]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
      "          \u001b[39m\u001b[22m\u001b[22md\u001b[35m:i32[10]\u001b[39m = pjit[\n",
      "            name=remainder\n",
      "            jaxpr={ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; e\u001b[35m:i32[10]\u001b[39m f\u001b[35m:i32[]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
      "                \u001b[39m\u001b[22m\u001b[22mg\u001b[35m:i32[]\u001b[39m = convert_element_type[new_dtype=int32 weak_type=False] f\n",
      "                h\u001b[35m:bool[]\u001b[39m = eq g 0\n",
      "                i\u001b[35m:i32[]\u001b[39m = pjit[\n",
      "                  name=_where\n",
      "                  jaxpr={ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; j\u001b[35m:bool[]\u001b[39m k\u001b[35m:i32[]\u001b[39m l\u001b[35m:i32[]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
      "                      \u001b[39m\u001b[22m\u001b[22mm\u001b[35m:i32[]\u001b[39m = select_n j l k\n",
      "                    \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(m,) }\n",
      "                ] h 1 g\n",
      "                n\u001b[35m:i32[10]\u001b[39m = rem e i\n",
      "                o\u001b[35m:bool[10]\u001b[39m = ne n 0\n",
      "                p\u001b[35m:bool[10]\u001b[39m = lt n 0\n",
      "                q\u001b[35m:bool[]\u001b[39m = lt i 0\n",
      "                r\u001b[35m:bool[10]\u001b[39m = ne p q\n",
      "                s\u001b[35m:bool[10]\u001b[39m = and r o\n",
      "                t\u001b[35m:i32[10]\u001b[39m = add n i\n",
      "                u\u001b[35m:i32[10]\u001b[39m = select_n s n t\n",
      "              \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(u,) }\n",
      "          ] c 2\n",
      "          v\u001b[35m:bool[10]\u001b[39m = eq d 0\n",
      "          w\u001b[35m:f32[10]\u001b[39m = convert_element_type[new_dtype=float32 weak_type=False] c\n",
      "          x\u001b[35m:f32[10]\u001b[39m = div w 2.0\n",
      "          y\u001b[35m:i32[10]\u001b[39m = mul 3 c\n",
      "          z\u001b[35m:i32[10]\u001b[39m = add y 1\n",
      "          ba\u001b[35m:f32[10]\u001b[39m = pjit[\n",
      "            name=_where\n",
      "            jaxpr={ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; bb\u001b[35m:bool[10]\u001b[39m bc\u001b[35m:f32[10]\u001b[39m bd\u001b[35m:i32[10]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
      "                \u001b[39m\u001b[22m\u001b[22mbe\u001b[35m:f32[10]\u001b[39m = convert_element_type[\n",
      "                  new_dtype=float32\n",
      "                  weak_type=False\n",
      "                ] bd\n",
      "                bf\u001b[35m:f32[10]\u001b[39m = select_n bb be bc\n",
      "              \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(bf,) }\n",
      "          ] v x z\n",
      "        \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(ba,) }\n",
      "    ] a\n",
      "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(b,) }\n"
     ]
    }
   ],
   "source": [
    "print(jax.make_jaxpr(myfunction)(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2369f925",
   "metadata": {},
   "source": [
    "### Automatic Differentiation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b215f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ddd43ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "def square(x):\n",
    "    return x ** 2\n",
    "\n",
    "print(square(10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "46946be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.0\n"
     ]
    }
   ],
   "source": [
    "print(grad(square)(10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9e5ce66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "print(grad(grad(square))(10.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabdf310",
   "metadata": {},
   "source": [
    "**Computing gradients in a linear logistic regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9fb69e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.key(7)\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 0.5 * (jnp.tanh(x / 2) + 1)\n",
    "\n",
    "# Outputs probability of a label being true.\n",
    "def predict(W, b, inputs):\n",
    "  return sigmoid(jnp.dot(inputs, W) + b)\n",
    "\n",
    "# Build a toy dataset.\n",
    "inputs = jnp.array([[0.52, 1.12,  0.77],\n",
    "                    [0.88, -1.08, 0.15],\n",
    "                    [0.52, 0.06, -1.30],\n",
    "                    [0.74, -2.49, 1.39]])\n",
    "targets = jnp.array([True, False, False, True])\n",
    "\n",
    "# Training loss is the negative log-likelihood of the training examples.\n",
    "def loss(W, b):\n",
    "  preds = predict(W, b, inputs)\n",
    "  label_probs = preds * targets + (1 - preds) * (1 - targets)\n",
    "  return -jnp.sum(jnp.log(label_probs))\n",
    "\n",
    "# Initialize random model coefficients\n",
    "key, W_key, b_key = jax.random.split(key, 3)\n",
    "W = jax.random.normal(W_key, (3,))\n",
    "b = jax.random.normal(b_key, ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "969e5024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_grad=Array([-0.5365673,  1.7310241, -2.2348125], dtype=float32)\n",
      "W_grad=Array([-0.5365673,  1.7310241, -2.2348125], dtype=float32)\n",
      "b_grad=Array(-0.7542623, dtype=float32)\n",
      "W_grad=Array([-0.5365673,  1.7310241, -2.2348125], dtype=float32)\n",
      "b_grad=Array(-0.7542623, dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Differentiate `loss` with respect to the first positional argument which is weight(W):\n",
    "W_grad = grad(loss, argnums=0)(W, b)\n",
    "print(f'{W_grad=}')\n",
    "\n",
    "# Since argnums=0 is the default, this does the same thing:\n",
    "W_grad = grad(loss)(W, b)\n",
    "print(f'{W_grad=}')\n",
    "\n",
    "# But you can choose different values too, and drop the keyword:\n",
    "b_grad = grad(loss, 1)(W, b)\n",
    "print(f'{b_grad=}')\n",
    "\n",
    "# Including tuple values\n",
    "W_grad, b_grad = grad(loss, (0, 1))(W, b)\n",
    "print(f'{W_grad=}')\n",
    "print(f'{b_grad=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524b6b7c",
   "metadata": {},
   "source": [
    "We can also perform  gradient checking and raises an error if the gradients are incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2b90f2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.test_util import check_grads\n",
    "\n",
    "check_grads(loss, (W, b), order=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b691a2",
   "metadata": {},
   "source": [
    "Now, this code will check if gradients of loss(W, b) are computed correctly up to the second order.\n",
    "\n",
    "And if we change the order to 4 or more than it will throw error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf87022c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0 3.0 4.0\n",
      "Printing derivatives with respect to both x and y, use argnums=(0,1) 3.0 4.0\n"
     ]
    }
   ],
   "source": [
    "f = lambda x,y: x**2 + x + 4 + y**2\n",
    "\n",
    "x=1.\n",
    "y=2.\n",
    "\n",
    "dfdx=grad(f,argnums=(0))\n",
    "dfdy=grad(f,argnums=(1))\n",
    "dfdx_dy=grad(f,argnums=(0,1))\n",
    "result=dfdx_dy(x,y)\n",
    "print(f(x,y),dfdx(x,y),dfdy(x,y))\n",
    "print(\"Printing derivatives with respect to both x and y, use argnums=(0,1)\",result[0].item() ,result[1].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ae491d",
   "metadata": {},
   "source": [
    "**Advanced automatic differentiation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "85e09c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian = (Array(3., dtype=float32, weak_type=True), Array(4., dtype=float32, weak_type=True))\n",
      "Full Hessian = ((Array(2., dtype=float32, weak_type=True), Array(0., dtype=float32, weak_type=True)), (Array(0., dtype=float32, weak_type=True), Array(2., dtype=float32, weak_type=True)))\n"
     ]
    }
   ],
   "source": [
    "from jax import jacfwd, jacrev\n",
    "\n",
    "def hessian(f):\n",
    "    return (jacfwd(jacrev(f, argnums=(0, 1)), argnums =(0, 1)))\n",
    "\n",
    "print(f'Jacobian = {jacrev(f, argnums=(0, 1))(1., 2.)}')\n",
    "print(f'Full Hessian = {hessian(f)(1.,2.)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7865299",
   "metadata": {},
   "source": [
    "### Automatic Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f5f71b",
   "metadata": {},
   "source": [
    "Automatic vectorization in JAX allows you to apply a function element-wise over batch dimensions efficiently, without writing explicit loops. This is done using jax.vmap, which transforms a function to operate on batches of inputs in parallel.\n",
    "\n",
    "**Why Use jax.vmap?**\n",
    "- Avoids writing explicit loops over batch dimensions.\n",
    "- Efficiently utilizes hardware acceleration (TPUs, GPUs).\n",
    "- Automatically optimizes execution for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "da40a359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.  1. 10.  2. 16.]\n"
     ]
    }
   ],
   "source": [
    "def my_function(x):\n",
    "    return jnp.where(x % 2 == 0, x / 2, 3 * x + 1)\n",
    "\n",
    "x = jnp.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Manually applying function to each element\n",
    "output = jnp.array([my_function(xi) for xi in x])\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9624592a",
   "metadata": {},
   "source": [
    "Works, but uses an explicit loop, which is inefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a5277477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.  1. 10.  2. 16.]\n"
     ]
    }
   ],
   "source": [
    "def my_function(x):\n",
    "    return jnp.where(x % 2 == 0, x / 2, 3 * x + 1)\n",
    "\n",
    "x = jnp.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Automatically vectorizing the function\n",
    "vectorized_function = jax.vmap(my_function)\n",
    "\n",
    "# Apply function to the entire array\n",
    "output = vectorized_function(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f818bc8",
   "metadata": {},
   "source": [
    "Faster and more efficient! JAX automatically applies the function over all elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248c055e",
   "metadata": {},
   "source": [
    "If the batch dimension is not the first, you may use the in_axes and out_axes arguments to specify the location of the batch dimension in inputs and outputs. These may be an integer if the batch axis is the same for all inputs and outputs, or lists, otherwise.More you can read here https://dinocausevic.com/2023/06/13/jax-vmap/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "45cd09d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47 71 99]\n",
      "[ 50 167]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import vmap\n",
    "\n",
    "# Define a dot product function\n",
    "A = jnp.array([[1, 2, 3], [4, 5, 6]])   # Shape (2,3)\n",
    "B = jnp.array([[7, 8, 9], [10, 11, 12]]) # Shape (2,3)\n",
    "\n",
    "dot_product = lambda x, y: jnp.dot(x, y) \n",
    "\n",
    "batched_dot = vmap(dot_product,(1,1)) \n",
    "output = batched_dot(A, B)\n",
    "\n",
    "print(output)  \n",
    "\n",
    "A = jnp.array([[1, 2, 3], [4, 5, 6]])   # Shape (2,3)\n",
    "B = jnp.array([[7, 8, 9], [10, 11, 12]]) # Shape (2,3)\n",
    "\n",
    "batched_dot = vmap(dot_product,(0,0)) \n",
    "output = batched_dot(A, B)\n",
    "\n",
    "print(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5421d40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47 71 99]\n"
     ]
    }
   ],
   "source": [
    "A = jnp.array([[1, 2, 3], [4, 5, 6]])   # Shape (2,3)\n",
    "B = jnp.array([[7, 8, 9], [10, 11, 12]]) # Shape (2,3)\n",
    "\n",
    "batched_dot = vmap(dot_product,(1,1)) \n",
    "output = batched_dot(A, B)\n",
    "\n",
    "print(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bf28effc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 50 122]\n",
      " [ 68 167]]\n"
     ]
    }
   ],
   "source": [
    "A = jnp.array([[1, 2, 3], [4, 5, 6]])   # Shape (2,3)\n",
    "B = jnp.array([[7, 8, 9], [10, 11, 12]]) # Shape (2,3)\n",
    "\n",
    "batched_dot = vmap(dot_product,(None,0)) \n",
    "output = batched_dot(A, B)\n",
    "\n",
    "print(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd89c354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 66  72  78]\n",
      " [156 171 186]]\n"
     ]
    }
   ],
   "source": [
    "A = jnp.array([[1, 2, 3], [4, 5, 6]])   # Shape (2,3)\n",
    "B = jnp.array([[7, 8, 9], [10, 11, 12],[13,14,15]]) # Shape (3,3)\n",
    "\n",
    "batched_dot = vmap(dot_product,(0,None),0) \n",
    "output = batched_dot(A, B)\n",
    "\n",
    "print(output)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4326d406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 66  72  78]\n",
      " [156 171 186]]\n"
     ]
    }
   ],
   "source": [
    "A = jnp.array([[1, 2, 3], [4, 5, 6]])   # Shape (2,3)\n",
    "B = jnp.array([[7, 8, 9], [10, 11, 12],[13,14,15]]) # Shape (3,3)\n",
    "\n",
    "batched_dot = vmap(dot_product,(None,1),1) \n",
    "output = batched_dot(A, B)\n",
    "\n",
    "print(output)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24209414",
   "metadata": {},
   "source": [
    "### Pseudorandom numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f4d176f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((), dtype=key<fry>) overlaying:\n",
       "[0 7]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "\n",
    "key=jax.random.key(7)\n",
    "key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e2686",
   "metadata": {},
   "source": [
    "A key is an array with a special dtype corresponding to the particular PRNG implementation being used; in the default implementation each key is backed by a pair of uint32 values.\n",
    "\n",
    "The key is effectively a stand-in for NumPy‚Äôs hidden state object, but we pass it explicitly to jax.random() functions. Importantly, random functions consume the key, but do not modify it: feeding the same key object to a random function will always result in the same sample being generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "137596f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45123515\n",
      "0.45123515\n"
     ]
    }
   ],
   "source": [
    "print(jax.random.normal(key))\n",
    "print(jax.random.normal(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3618f4d",
   "metadata": {},
   "source": [
    "Re-using the same key, even with different random APIs, can result in correlated outputs, which is generally undesirable.\n",
    "\n",
    "The rule of thumb is: never reuse keys (unless you want identical outputs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931b8bfd",
   "metadata": {},
   "source": [
    "JAX uses a modern Threefry counter-based PRNG that‚Äôs splittable. That is, its design allows us to fork the PRNG state into new PRNGs for use with parallel stochastic generation. In order to generate different and independent samples, you must split() the key explicitly before passing it to a random function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5fd85d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "draw 0: 0.4541335105895996\n",
      "draw 1: 2.2433922290802\n",
      "draw 2: 1.0064350366592407\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "  new_key, subkey = jax.random.split(key)\n",
    "  del key  # The old key is consumed by split() -- we must never use it again.\n",
    "\n",
    "  val = jax.random.normal(subkey)\n",
    "  del subkey  # The subkey is consumed by normal().\n",
    "\n",
    "  print(f\"draw {i}: {val}\")\n",
    "  key = new_key  # new_key is safe to use in the next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace8c6f7",
   "metadata": {},
   "source": [
    "(Calling del here is not required, but we do so to emphasize that the key should not be reused once consumed.)\n",
    "\n",
    "jax.random.split() is a deterministic function that converts one key into several independent (in the pseudorandomness sense) keys. We keep one of the outputs as the new_key, and can safely use the unique extra key (called subkey) as input into a random function, and then discard it forever. If you wanted to get another sample from the normal distribution, you would split key again, and so on: the crucial point is that you never use the same key twice.\n",
    "\n",
    "It doesn‚Äôt matter which part of the output of split(key) we call key, and which we call subkey. They are all independent keys with equal status. The key/subkey naming convention is a typical usage pattern that helps track how keys are consumed: subkeys are destined for immediate consumption by random functions, while the key is retained to generate more randomness later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d33df3",
   "metadata": {},
   "source": [
    "Usually, the above example would be written concisely as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "805c32dc",
   "metadata": {},
   "outputs": [],
   "source": [
    ".key, subkey = jax.random.split(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55b504c",
   "metadata": {},
   "source": [
    "which discards the old key automatically. It‚Äôs worth noting that split() can create as many keys as you need, not just 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c4e673b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array((5,), dtype=key<fry>) overlaying:\n",
      "[[1655368132  475302476]\n",
      " [ 400014582 1736700117]\n",
      " [1856950286 2089119175]\n",
      " [4124452448  828469155]\n",
      " [2409566084 2032918171]]\n"
     ]
    }
   ],
   "source": [
    "keys=jax.random.split(key,5)\n",
    "\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c693445",
   "metadata": {},
   "source": [
    "### Pytrees\n",
    "\n",
    "A pytree is a container-like structure built out of container-like Python objects ‚Äî ‚Äúleaf‚Äù pytrees and/or more pytrees. A pytree can include lists, tuples, and dicts. A leaf is anything that‚Äôs not a pytree, such as an array, but a single leaf is also a pytree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5cd003",
   "metadata": {},
   "source": [
    "In JAX, you can use jax.tree.leaves(), to extract the flattened leaves from the trees, as demonstrated here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3810e029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 'a', <object object at 0x000001E3E63316E0>] has 3 leaves: [1, 'a', <object object at 0x000001E3E63316E0>]\n",
      "(1, (2, 3), ())                               has 3 leaves: [1, 2, 3]\n",
      "[1, {'k1': 2, 'k2': (3, 4)}, 5]               has 5 leaves: [1, 2, 3, 4, 5]\n",
      "{'a': 2, 'b': (2, 3)}                         has 3 leaves: [2, 2, 3]\n",
      "Array([1, 2, 3], dtype=int32)                 has 1 leaves: [Array([1, 2, 3], dtype=int32)]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "example_trees = [\n",
    "    [1, 'a', object()],\n",
    "    (1, (2, 3), ()),\n",
    "    [1, {'k1': 2, 'k2': (3, 4)}, 5],\n",
    "    {'a': 2, 'b': (2, 3)},\n",
    "    jnp.array([1, 2, 3]),\n",
    "]\n",
    "\n",
    "# Print how many leaves the pytrees have.\n",
    "for pytree in example_trees:\n",
    "  # This `jax.tree.leaves()` method extracts the flattened leaves from the pytrees.\n",
    "  leaves = jax.tree.leaves(pytree)\n",
    "  print(f\"{repr(pytree):<45} has {len(leaves)} leaves: {leaves}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdcbd6a",
   "metadata": {},
   "source": [
    "**jax.tree.map**\n",
    "\n",
    "jax.tree_map is a  function in JAX that applies a function element-wise to all leaves of a PyTree (nested structure). It is similar to Python's map(), but works on nested data structures like lists, tuples, and dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb91cda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': Array([2, 4, 6], dtype=int32), 'b': [8, 10, 12]}\n"
     ]
    }
   ],
   "source": [
    "tree = {'a': jnp.array([1, 2, 3]), 'b': [4, 5, 6]}\n",
    "result = jax.tree.map(lambda x: x * 2, tree)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a970cd",
   "metadata": {},
   "source": [
    "**Extending PyTrees with jax.tree_util.register_pytree_node() and jax.tree_map()**\n",
    "\n",
    "JAX provides PyTrees (nested data structures) to handle complex data efficiently. By default, JAX treats Python structures like lists, tuples, dictionaries, and NumPy arrays as PyTrees.\n",
    "\n",
    "However, custom classes are not automatically recognized as PyTrees.\n",
    "To extend JAX's PyTree system for a custom class, we use:\n",
    "\n",
    "jax.tree_util.register_pytree_node() ‚Üí Registers a custom object as a PyTree.\n",
    "jax.tree_map() ‚Üí Applies a function to PyTree leaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27148acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Special at 0x1e3e482cb90>, <__main__.Special at 0x1e3e6b4ff10>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Special(object):\n",
    "  def __init__(self, x, y):\n",
    "    self.x = x\n",
    "    self.y = y\n",
    "\n",
    "jax.tree.leaves([\n",
    "    Special(0, 1),\n",
    "    Special(2, 4),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc9eb9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.tree_util import register_pytree_node\n",
    "\n",
    "class RegisteredSpecial(Special):\n",
    "  def __repr__(self):\n",
    "    return \"RegisteredSpecial(x={}, y={})\".format(self.x, self.y)\n",
    "\n",
    "def special_flatten(v):\n",
    "  \"\"\"Specifies a flattening recipe.\n",
    "\n",
    "  Params:\n",
    "    v: The value of the registered type to flatten.\n",
    "  Returns:\n",
    "    A pair of an iterable with the children to be flattened recursively,\n",
    "    and some opaque auxiliary data to pass back to the unflattening recipe.\n",
    "    The auxiliary data is stored in the treedef for use during unflattening.\n",
    "    The auxiliary data could be used, for example, for dictionary keys.\n",
    "  \"\"\"\n",
    "  children = (v.x, v.y)\n",
    "  aux_data = None\n",
    "  return (children, aux_data)\n",
    "\n",
    "def special_unflatten(aux_data, children):\n",
    "  \"\"\"Specifies an unflattening recipe.\n",
    "\n",
    "  Params:\n",
    "    aux_data: The opaque data that was specified during flattening of the\n",
    "      current tree definition.\n",
    "    children: The unflattened children\n",
    "\n",
    "  Returns:\n",
    "    A reconstructed object of the registered type, using the specified\n",
    "    children and auxiliary data.\n",
    "  \"\"\"\n",
    "  return RegisteredSpecial(*children)\n",
    "\n",
    "# Global registration\n",
    "register_pytree_node(\n",
    "    RegisteredSpecial,\n",
    "    special_flatten,    # Instruct JAX what are the children nodes.\n",
    "    special_unflatten   # Instruct JAX how to pack back into a `RegisteredSpecial`.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fc9ac65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RegisteredSpecial(x=1, y=2), RegisteredSpecial(x=3, y=5)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree.map(lambda x: x + 1,\n",
    "  [\n",
    "   RegisteredSpecial(0, 1),\n",
    "   RegisteredSpecial(2, 4),\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad75c369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "203d9766",
   "metadata": {},
   "source": [
    "### Gotcha #1: Pure functions\n",
    "- JAX is designed to work only on pure functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38042ed",
   "metadata": {},
   "source": [
    "A pure function is a function that satisfies the following two properties:\n",
    "\n",
    "- No Side Effects ‚Äì The function does not modify any external state (e.g., global variables, I/O operations, database writes).\n",
    "- Deterministic Output ‚Äì For the same input, the function always produces the same output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696654e6",
   "metadata": {},
   "source": [
    "**Why Does JAX Require Pure Functions?**\n",
    "\n",
    "JAX uses just-in-time (JIT) compilation and automatic differentiation. If functions have side effects, JAX cannot safely optimize or parallelize computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e3ade0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing function\n",
      "First call:  4.0\n",
      "Second call:  5.0\n",
      "Executing function\n",
      "Third call, different type:  [5.]\n"
     ]
    }
   ],
   "source": [
    "#Example 1\n",
    "\n",
    "def impure_print_side_effect(x):\n",
    "    print(\"Executing function\") # Side effect: printing\n",
    "    return x\n",
    "\n",
    "# The side-effects appear during the first run \n",
    "print (\"First call: \", jit(impure_print_side_effect)(4.))\n",
    "\n",
    "# Subsequent runs with parameters of same type and shape may not show the side-effect\n",
    "# This is because JAX now invokes a cached compiled version of the function\n",
    "print (\"Second call: \", jit(impure_print_side_effect)(5.))\n",
    "\n",
    "# JAX re-runs the Python function when the type or shape of the argument changes\n",
    "print (\"Third call, different type: \", jit(impure_print_side_effect)(jnp.array([5.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be1bdf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call:  4.0\n",
      "Second call:  5.0\n",
      "Third call, different type:  [14.]\n"
     ]
    }
   ],
   "source": [
    "# Example 2\n",
    "\n",
    "g = 0.\n",
    "\n",
    "def impure_uses_globals(x):\n",
    "    return x + g # Modifies external variable\n",
    "\n",
    "# JAX captures the value of the global during the first run\n",
    "print (\"First call: \", jit(impure_uses_globals)(4.))\n",
    "\n",
    "# Let's update the global!\n",
    "g = 10.\n",
    "\n",
    "# Subsequent runs may silently use the cached value of the globals\n",
    "print (\"Second call: \", jit(impure_uses_globals)(5.))\n",
    "\n",
    "# JAX re-runs the Python function when the type or shape of the argument changes\n",
    "# This will end up reading the latest value of the global\n",
    "print (\"Third call, different type: \", jit(impure_uses_globals)(jnp.array([4.])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb050d7",
   "metadata": {},
   "source": [
    "### Gotcha #2: In-Place Updates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36e8cf6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original array unchanged:\n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "updated array:\n",
      " [[0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "jax_array = jnp.zeros((3,3), dtype=jnp.float32)\n",
    "updated_array = jax_array.at[1, :].set(1.0)\n",
    "\n",
    "print(\"original array unchanged:\\n\", jax_array)\n",
    "print(\"updated array:\\n\", updated_array)\n",
    "\n",
    "# If this seems wasteful to you, congrats, you did some algorithms/data structures in your life. ;)\n",
    "# The thing is - inside jit-compiled code, if the input value x of x.at[idx].set(y) \n",
    "# is not reused, the compiler will optimize the array update to occur in-place!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc5285a",
   "metadata": {},
   "source": [
    "### Gotcha #3: Out-of-Bounds Indexing\n",
    "Due to JAX's accelerator agnostic approach JAX had to make a non-error behaviour for out of bounds indexing (similarly to how invalid fp arithmetic results in NaNs and not an exception)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df4927e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception index 11 is out of bounds for axis 0 with size 10\n"
     ]
    }
   ],
   "source": [
    "# NumPy behavior\n",
    "import numpy as np \n",
    "try:\n",
    "  np.arange(10)[11]\n",
    "except Exception as e:\n",
    "    print(\"Exception {}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "943debe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# JAX behavior\n",
    "# 1) updates at out-of-bounds indices are skipped\n",
    "# 2) retrievals result in index being clamped\n",
    "# in general there are currently some bugs so just consider the behavior undefined!\n",
    "\n",
    "print(jnp.arange(10).at[11].add(23))  # example of 1)\n",
    "print(jnp.arange(10)[11])  # example of 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7408824c",
   "metadata": {},
   "source": [
    "### Stateful computations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c40c3de",
   "metadata": {},
   "source": [
    "JAX transformations like jit(), vmap(), grad(), require the functions they wrap to be pure: that is, functions whose outputs depend solely on the inputs, and which have no side effects such as updating of global state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd900264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class Counter:\n",
    "  \"\"\"A simple counter.\"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    self.n = 0\n",
    "\n",
    "  def count(self) -> int:\n",
    "    \"\"\"Increments the counter and returns the new value.\"\"\"\n",
    "    self.n += 1\n",
    "    return self.n\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"Resets the counter to zero.\"\"\"\n",
    "    self.n = 0\n",
    "\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "for _ in range(3):\n",
    "  print(counter.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3a27d7",
   "metadata": {},
   "source": [
    "The counter‚Äôs n attribute maintains the counter‚Äôs state between successive calls of count. It is modified as a side effect of calling count.\n",
    "\n",
    "Let‚Äôs say we want to count fast, so we JIT-compile the count method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "117b2a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "counter.reset()\n",
    "fast_count = jax.jit(counter.count)\n",
    "\n",
    "for _ in range(3):\n",
    "  print(fast_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef10ee0a",
   "metadata": {},
   "source": [
    "Oh no! Our counter isn‚Äôt working. This is because the line:\n",
    "\n",
    "self.n += 1\n",
    "\n",
    "in count involves a side effect: it modifies the input counter in-place, and so this function is not supported by jit. Such side effects are executed only once when the function is first traced, and subsequent calls will not repeat the side effect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ccdece",
   "metadata": {},
   "source": [
    "**The solution: explicit state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca7ca797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "CounterState = int\n",
    "\n",
    "class CounterV2:\n",
    "\n",
    "  def count(self, n: CounterState) -> tuple[int, CounterState]:\n",
    "    # You could just return n+1, but here we separate its role as \n",
    "    # the output and as the counter state for didactic purposes.\n",
    "    return n+1, n+1\n",
    "\n",
    "  def reset(self) -> CounterState:\n",
    "    return 0\n",
    "\n",
    "counter = CounterV2()\n",
    "state = counter.reset()\n",
    "\n",
    "for _ in range(3):\n",
    "  value, state = counter.count(state)\n",
    "  print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2035c5ce",
   "metadata": {},
   "source": [
    "In this new version of Counter, we moved n to be an argument of count, and added another return value that represents the new, updated, state. To use this counter, we now need to keep track of the state explicitly. But in return, we can now safely jax.jit this counter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "146c8a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "state = counter.reset()\n",
    "fast_count = jax.jit(counter.count)\n",
    "\n",
    "for _ in range(3):\n",
    "  value, state = fast_count(state)\n",
    "  print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1747517b",
   "metadata": {},
   "source": [
    "**A general strategy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fda754",
   "metadata": {},
   "source": [
    "We can apply the same process to any stateful method to convert it into a stateless one. We took a class of the form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8929c6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatefulClass\n",
    "\n",
    "  state: State\n",
    "\n",
    "  def stateful_method(*args, **kwargs) -> Output:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fa79eb",
   "metadata": {},
   "source": [
    "and turned it into a class of the form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad221fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatelessClass\n",
    "\n",
    "  def stateless_method(state: State, *args, **kwargs) -> (Output, State):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b698df",
   "metadata": {},
   "source": [
    "### Taining a toy MLP (multi-layer perceptron) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38bd06c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'biases': (128,), 'weights': (1, 128)},\n",
       " {'biases': (128,), 'weights': (128, 128)},\n",
       " {'biases': (1,), 'weights': (128, 1)}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def init_mlp_params(layer_widths):\n",
    "    params = []\n",
    "\n",
    "    # Allocate weights and biases (model parameters)\n",
    "    # Notice: we're not using JAX's PRNG here - doesn't matter for this simple example\n",
    "    for n_in, n_out in zip(layer_widths[:-1], layer_widths[1:]):\n",
    "        params.append(\n",
    "            dict(weights=np.random.normal(size=(n_in, n_out)) * np.sqrt(2/n_in),\n",
    "                biases=np.ones(shape=(n_out,))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return params\n",
    "\n",
    "# Instantiate a single input - single output, 3 layer (2 hidden layers) deep MLP\n",
    "params = init_mlp_params([1, 128, 128, 1])\n",
    "\n",
    "# Another example of how we might use tree_map - verify that shapes make sense:\n",
    "jax.tree_map(lambda x: x.shape, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11d2167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(params, x):\n",
    "    *hidden, last = params\n",
    "\n",
    "    for layer in hidden:\n",
    "        x = jax.nn.relu(jnp.dot(x, layer['weights']) + layer['biases'])\n",
    "\n",
    "    return jnp.dot(x, last['weights']) + last['biases']\n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "    return jnp.mean((forward(params, x) - y) ** 2)  # MSE loss\n",
    "\n",
    "lr = 0.0001\n",
    "\n",
    "@jit  # notice how we do jit only at the highest level - XLA will have plenty of space to optimize\n",
    "def update(params, x, y):\n",
    "\n",
    "    # Note that grads is a pytree with the same structure as params.\n",
    "    # grad is one of the many JAX functions that has built-in support for pytrees!\n",
    "    grads = jax.grad(loss_fn)(params, x, y)\n",
    "\n",
    "    # SGD update\n",
    "    return jax.tree_map(\n",
    "        lambda p, g: p - lr * g, params, grads  # for every leaf i.e. for every param of MLP\n",
    "    )\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3a44fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = np.random.normal(size=(128, 1))\n",
    "ys = xs ** 2  # let's learn how to regress a parabola\n",
    "\n",
    "num_epochs = 5000\n",
    "for _ in range(num_epochs):\n",
    "    params = update(params, xs, ys)  # again our lovely pattern\n",
    "\n",
    "plt.scatter(xs, ys)\n",
    "plt.scatter(xs, forward(params, xs), label='Model prediction')\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
